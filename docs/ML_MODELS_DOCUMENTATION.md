# ðŸ¤– Heal-X-Bot: ML Models Documentation

## Table of Contents

1. [Overview](#overview)
2. [DDoS Detection Model (TensorFlow)](#ddos-detection-model-tensorflow)
3. [Predictive Maintenance Model (XGBoost)](#predictive-maintenance-model-xgboost)
4. [Model Integration](#model-integration)
5. [Model Monitoring](#model-monitoring)
6. [Visualizations](#visualizations)
7. [Performance Metrics](#performance-metrics)
8. [Training & Retraining](#training--retraining)

---

## Overview

Heal-X-Bot employs **two advanced machine learning models** to provide comprehensive threat detection and system health prediction:

1. **DDoS Detection Model** - Deep learning model using TensorFlow/Keras for real-time attack detection
2. **Predictive Maintenance Model** - Gradient boosting model using XGBoost for failure prediction

Both models work together to provide proactive security and system management capabilities.

### Model Comparison

| Feature | DDoS Detection | Predictive Maintenance |
|---------|---------------|----------------------|
| **Framework** | TensorFlow/Keras | XGBoost |
| **Type** | Deep Neural Network | Gradient Boosting |
| **Purpose** | Threat Detection | Failure Prediction |
| **Input** | Network Traffic Features | System Metrics |
| **Output** | Attack Probability (0-1) | Risk Score + Time-to-Failure |
| **Update Frequency** | Real-time | Every 60 seconds |
| **Prediction Horizon** | Immediate | 1-24 hours ahead |

---

## DDoS Detection Model (TensorFlow)

### Model Architecture

The DDoS detection model is a **deep neural network** built with TensorFlow/Keras that analyzes network traffic patterns to identify potential DDoS attacks.

#### Architecture Details

```
Input Layer (28 features)
    â”‚
    â–¼
Dense Layer 1 (128 neurons, ReLU activation)
    â”‚
    â–¼
Dropout Layer (0.3 dropout rate)
    â”‚
    â–¼
Dense Layer 2 (64 neurons, ReLU activation)
    â”‚
    â–¼
Dropout Layer (0.3 dropout rate)
    â”‚
    â–¼
Dense Layer 3 (32 neurons, ReLU activation)
    â”‚
    â–¼
Output Layer (1 neuron, Sigmoid activation)
    â”‚
    â–¼
DDoS Probability (0.0 - 1.0)
```

#### Model Specifications

- **Input Features**: 28 network traffic features
- **Hidden Layers**: 3 fully connected layers
- **Activation Functions**: ReLU (hidden), Sigmoid (output)
- **Regularization**: Dropout (30% rate)
- **Optimizer**: Adam
- **Loss Function**: Binary Cross-Entropy
- **Model Format**: Keras (.keras)

### Feature Engineering

The model analyzes **28 key network traffic features**:

#### Protocol & Flow Features
1. **Protocol** - Network protocol type
2. **Flow Duration** - Duration of network flow
3. **Total Forward Packets** - Count of forward packets
4. **Total Backward Packets** - Count of backward packets

#### Packet Length Features
5. **Forward Packet Length Mean** - Average forward packet size
6. **Backward Packet Length Mean** - Average backward packet size

#### Inter-Arrival Time (IAT) Features
7. **Flow IAT Mean** - Mean inter-arrival time for flow
8. **Flow IAT Std** - Standard deviation of IAT
9. **Flow IAT Max** - Maximum IAT
10. **Flow IAT Min** - Minimum IAT
11. **Forward IAT Mean** - Mean forward IAT
12. **Forward IAT Std** - Forward IAT standard deviation
13. **Forward IAT Max** - Maximum forward IAT
14. **Forward IAT Min** - Minimum forward IAT
15. **Backward IAT Mean** - Mean backward IAT
16. **Backward IAT Std** - Backward IAT standard deviation
17. **Backward IAT Max** - Maximum backward IAT
18. **Backward IAT Min** - Minimum backward IAT

#### Active/Idle Time Features
19. **Active Mean** - Mean active time
20. **Active Std** - Active time standard deviation
21. **Active Max** - Maximum active time
22. **Active Min** - Minimum active time
23. **Idle Mean** - Mean idle time
24. **Idle Std** - Idle time standard deviation
25. **Idle Max** - Maximum idle time
26. **Idle Min** - Minimum idle time

#### Feature Normalization

All features are normalized using **StandardScaler**:
- Mean = 0
- Standard Deviation = 1
- Applied before prediction

### Training Process

#### Training Data

- **Source**: Network traffic datasets with labeled DDoS attacks
- **Attack Types**: HTTP Flood, SYN Flood, Bot Activity, UDP Flood
- **Data Split**: 80% training, 20% validation
- **Augmentation**: Synthetic attack pattern generation

#### Training Parameters

- **Epochs**: 50-100 (with early stopping)
- **Batch Size**: 32-64
- **Learning Rate**: 0.001 (Adam optimizer)
- **Validation Split**: 20%
- **Early Stopping**: Patience = 10 epochs

#### Training Output

The model generates:
- Trained model file: `ddos_model.keras`
- Training history (loss, accuracy)
- Validation metrics
- Feature importance analysis

### Prediction Process

#### Real-Time Prediction Flow

```
Network Traffic Alert
    â”‚
    â–¼
Feature Extraction (28 features)
    â”‚
    â–¼
Feature Normalization (StandardScaler)
    â”‚
    â–¼
Model Prediction (Neural Network)
    â”‚
    â–¼
DDoS Probability (0.0 - 1.0)
    â”‚
    â–¼
Threat Level Assessment
    â”‚
    â”œâ”€ Probability < 0.5 â†’ Legitimate Traffic
    â”œâ”€ Probability 0.5-0.8 â†’ Medium Threat
    â””â”€ Probability > 0.8 â†’ High Threat (Auto-Block)
```

#### Prediction Output

```json
{
  "timestamp": "20250119_120000",
  "prediction": 0.87,
  "is_ddos": true,
  "confidence": 0.74,
  "risk_level": "High",
  "confidence_level": "Medium",
  "trend": "Increasing",
  "feature_values": {
    "Protocol": 0.5,
    "Flow Duration": 0.3,
    ...
  }
}
```

### Performance Metrics

#### Typical Performance

- **Accuracy**: 92-95%
- **Precision**: 88-92%
- **Recall**: 90-94%
- **F1-Score**: 89-93%
- **False Positive Rate**: < 5%
- **Prediction Latency**: < 50ms

#### Attack Type Detection Rates

| Attack Type | Detection Rate |
|------------|----------------|
| HTTP Flood | 94-96% |
| SYN Flood | 92-95% |
| Bot Activity | 90-93% |
| UDP Flood | 91-94% |
| Mixed Attacks | 88-92% |

### Visualizations

#### 1. Feature Importance Chart

**Description**: Horizontal bar chart showing normalized feature values for a prediction.

**Visualization Details**:
- **Type**: Horizontal Bar Chart
- **X-Axis**: Normalized Feature Value (-3 to +3)
- **Y-Axis**: Feature Names (28 features)
- **Colors**: Gradient from blue (low) to red (high)
- **Size**: 12x6 inches

**Interpretation**:
- Features with high absolute values are most influential
- Positive values indicate attack-like patterns
- Negative values indicate normal traffic patterns

**Location**: `model/visualizations/feature_importance_{timestamp}.png`

#### 2. Prediction Trend Chart

**Description**: Line chart showing DDoS prediction probability over time.

**Visualization Details**:
- **Type**: Line Chart with Markers
- **X-Axis**: Timestamp
- **Y-Axis**: Detection Probability (0.0 - 1.0)
- **Reference Line**: Decision Threshold at 0.5 (red dashed line)
- **Size**: 12x6 inches

**Interpretation**:
- Values above 0.5 indicate DDoS attack
- Increasing trend suggests ongoing attack
- Spikes indicate attack bursts

**Location**: `model/visualizations/prediction_trend.png`

#### 3. Confidence Distribution Chart

**Description**: Histogram showing distribution of prediction confidence scores.

**Visualization Details**:
- **Type**: Histogram with KDE (Kernel Density Estimation)
- **X-Axis**: Confidence Score (0.0 - 1.0)
- **Y-Axis**: Frequency
- **Bins**: 20 bins
- **Size**: 10x6 inches

**Interpretation**:
- High confidence scores (> 0.8) indicate reliable predictions
- Bimodal distribution suggests clear separation between attacks and normal traffic
- Low confidence scores may indicate ambiguous patterns

**Location**: `model/visualizations/confidence_distribution.png`

---

## Predictive Maintenance Model (XGBoost)

### Model Architecture

The predictive maintenance model uses **XGBoost** (Gradient Boosting) with a **dual model approach**:

1. **Classification Model** - Predicts if a failure will occur (binary)
2. **Regression Model** - Predicts time until failure (continuous, hours)

#### Model Specifications

**Classification Model**:
- **Algorithm**: XGBoost Classifier
- **Objective**: Binary Logistic
- **Max Depth**: 6-8
- **Learning Rate**: 0.1
- **N Estimators**: 100-200
- **Output**: Failure Probability (0.0 - 1.0)

**Regression Model**:
- **Algorithm**: XGBoost Regressor
- **Objective**: Reg:squarederror
- **Max Depth**: 6-8
- **Learning Rate**: 0.1
- **N Estimators**: 100-200
- **Output**: Hours Until Failure (0-24+)

### Feature Engineering

The model uses **145 engineered features** from system metrics and log patterns:

#### System Metrics (8 base features)
1. **CPU Percent** - CPU usage percentage
2. **Memory Percent** - Memory usage percentage
3. **Disk Percent** - Disk usage percentage
4. **Network In Bytes** - Incoming network traffic
5. **Network Out Bytes** - Outgoing network traffic
6. **Connections Count** - Active network connections
7. **Memory Available GB** - Available memory
8. **Disk Free GB** - Free disk space

#### Log Pattern Features (6 base features)
9. **Error Count** - Number of error-level log entries
10. **Warning Count** - Number of warning-level log entries
11. **Critical Count** - Number of critical-level log entries
12. **Service Failures** - Service failure indicators
13. **Auth Failures** - Authentication failure count
14. **SSH Attempts** - SSH connection attempts

#### Time-Windowed Aggregations

For each base feature, the model creates:
- **Current Value**
- **Mean** (1h, 6h, 24h windows)
- **Std** (1h, 6h, 24h windows)
- **Min** (1h, 6h, 24h windows)
- **Max** (1h, 6h, 24h windows)
- **Trend** (slope over 1h, 6h, 24h)

**Total Features**: 14 base features Ã— ~10 aggregations = **~140 features**

#### Predictive Indicators (28 features)

Additional engineered features:
- **Resource Degradation Rate** - Rate of resource exhaustion
- **Error Escalation** - Increasing error frequency
- **Service Health Trend** - Service status trends
- **Network Anomaly Score** - Network pattern anomalies
- **Disk Exhaustion Rate** - Rate of disk space consumption
- **Memory Leak Indicators** - Memory usage patterns
- **CPU Spike Frequency** - Frequency of CPU spikes
- **Connection Anomaly** - Unusual connection patterns

### Training Process

#### Training Data

**Data Sources**:
1. **System Logs** - Centralized log files
2. **Monitoring API** - Historical metrics
3. **Synthetic Data** - Generated failure patterns (if needed)

**Data Generation**:
- **Synthetic Data**: 10,000+ samples with failure patterns
- **Failure Rate**: 5% of samples contain failures
- **Temporal Patterns**: Degradation patterns before failures
- **Time Windows**: 1, 6, 24 hours prediction horizons

#### Training Parameters

**Classification Model**:
- **Test Size**: 20%
- **Time Series Split**: 5 folds
- **Hyperparameter Tuning**: RandomizedSearchCV (20-50 iterations)
- **Early Stopping**: 10 rounds

**Regression Model**:
- **Test Size**: 20%
- **Time Series Split**: 5 folds
- **Hyperparameter Tuning**: RandomizedSearchCV (20-50 iterations)
- **Early Stopping**: 10 rounds

#### Training Output

The training process generates:

**Model Files**:
- `model.json` or `model.pkl` - Classification model
- `regression_model.json` or `regression_model.pkl` - Regression model
- `scaler.pkl` - Feature scaler
- `feature_names.json` - Feature names list

**Metrics Files**:
- `metrics.json` - Comprehensive evaluation metrics
- `config.json` - Training configuration
- `prediction_thresholds.json` - Optimal thresholds

**Visualizations**:
- `confusion_matrix.png` - Classification confusion matrix
- `roc_curve.png` - ROC curve
- `feature_importance.png` - Feature importance chart
- `prediction_timeline.png` - Prediction timeline visualization

### Prediction Process

#### Real-Time Prediction Flow

```
System Metrics Collection (Every 60s)
    â”‚
    â–¼
Feature Engineering (145 features)
    â”‚
    â–¼
Feature Scaling (StandardScaler)
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚
    â–¼                 â–¼
Classification    Regression
Model             Model
    â”‚                 â”‚
    â–¼                 â–¼
Failure Risk      Time Until
(0.0 - 1.0)       Failure (hours)
    â”‚                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    Early Warning Indicators
             â”‚
             â–¼
    Dashboard Alert
```

#### Prediction Output

```json
{
  "timestamp": "2025-01-19T12:00:00",
  "risk_score": 0.75,
  "risk_percentage": 75.0,
  "has_early_warning": true,
  "is_high_risk": true,
  "hours_until_failure": 4.5,
  "predicted_failure_time": "2025-01-19T16:30:00",
  "confidence": "High",
  "early_warnings": [
    {
      "type": "cpu_high",
      "severity": "high",
      "message": "CPU usage at 92.5%"
    },
    {
      "type": "memory_high",
      "severity": "high",
      "message": "Memory usage at 88.0%"
    }
  ]
}
```

### Performance Metrics

#### Classification Model Performance

**Typical Performance**:
- **Accuracy**: 92-95%
- **Precision**: 88-92%
- **Recall**: 85-90%
- **F1-Score**: 86-91%
- **ROC-AUC**: 0.94-0.96
- **PR-AUC**: 0.90-0.93

**Predictive Performance**:
- **Early Detection Rate**: 80-90% (failures predicted >1 hour before)
- **Mean Lead Time**: 3-6 hours (average prediction lead time)
- **Median Lead Time**: 2-5 hours
- **False Positive Rate**: < 10%

#### Regression Model Performance

**Typical Performance**:
- **MAE** (Mean Absolute Error): 1.5-3.0 hours
- **RMSE** (Root Mean Squared Error): 2.0-4.0 hours
- **Accuracy within 1 hour**: 70-80%
- **Accuracy within 2 hours**: 85-92%
- **RÂ² Score**: 0.85-0.92

#### Prediction Horizon Performance

| Horizon | Early Detection Rate | Mean Lead Time |
|---------|---------------------|----------------|
| 1 hour | 85-90% | 0.5-1.0 hours |
| 6 hours | 80-85% | 2-4 hours |
| 24 hours | 75-80% | 4-8 hours |

### Visualizations

#### 1. Confusion Matrix

**Description**: Matrix showing classification performance (True/False Positives/Negatives).

**Visualization Details**:
- **Type**: Heatmap Matrix
- **Dimensions**: 2x2 (Predicted vs Actual)
- **Colors**: Gradient from blue (low) to red (high)
- **Annotations**: Count and percentage in each cell
- **Size**: 8x6 inches

**Interpretation**:
- **True Negatives** (top-left): Correctly predicted no failure
- **False Positives** (top-right): Predicted failure but didn't occur
- **False Negatives** (bottom-left): Missed failure prediction
- **True Positives** (bottom-right): Correctly predicted failure

**Location**: `model/artifacts/{version}/plots/confusion_matrix.png`

#### 2. ROC Curve

**Description**: Receiver Operating Characteristic curve showing true positive rate vs false positive rate.

**Visualization Details**:
- **Type**: Line Chart
- **X-Axis**: False Positive Rate (0.0 - 1.0)
- **Y-Axis**: True Positive Rate (0.0 - 1.0)
- **Reference Line**: Random classifier (diagonal)
- **AUC Score**: Displayed in legend
- **Size**: 8x6 inches

**Interpretation**:
- **AUC > 0.9**: Excellent model
- **AUC 0.8-0.9**: Good model
- **AUC < 0.8**: Needs improvement
- Curve closer to top-left = better performance

**Location**: `model/artifacts/{version}/plots/roc_curve.png`

#### 3. Feature Importance Chart

**Description**: Bar chart showing top 20 most important features for predictions.

**Visualization Details**:
- **Type**: Horizontal Bar Chart
- **X-Axis**: Feature Importance Score
- **Y-Axis**: Feature Names (top 20)
- **Colors**: Gradient from blue (low) to red (high)
- **Size**: 12x8 inches

**Interpretation**:
- Higher scores = more important features
- Top features drive most predictions
- Can identify key failure indicators

**Location**: `model/artifacts/{version}/plots/feature_importance.png`

#### 4. Prediction Timeline

**Description**: Timeline visualization showing actual failures vs predicted failures over time.

**Visualization Details**:
- **Type**: Line Chart with Markers
- **X-Axis**: Time (timestamps)
- **Y-Axis**: Risk Score (0.0 - 1.0)
- **Lines**: 
  - Actual failures (red vertical lines)
  - Predicted risk (blue line)
  - Threshold (orange dashed line)
- **Size**: 14x6 inches

**Interpretation**:
- Risk score increases before actual failures
- Predictions should precede actual failures
- Gaps indicate missed predictions

**Location**: `model/artifacts/{version}/plots/prediction_timeline.png`

#### 5. Precision-Recall Curve

**Description**: Curve showing precision vs recall at different thresholds.

**Visualization Details**:
- **Type**: Line Chart
- **X-Axis**: Recall (0.0 - 1.0)
- **Y-Axis**: Precision (0.0 - 1.0)
- **AP Score**: Average Precision displayed
- **Size**: 8x6 inches

**Interpretation**:
- Higher curve = better performance
- AP > 0.9 = excellent
- Useful for imbalanced datasets

**Location**: `model/artifacts/{version}/plots/precision_recall_curve.png`

#### 6. Lead Time Distribution

**Description**: Histogram showing distribution of prediction lead times (hours before failure).

**Visualization Details**:
- **Type**: Histogram
- **X-Axis**: Lead Time (hours)
- **Y-Axis**: Frequency
- **Statistics**: Mean, median displayed
- **Size**: 10x6 inches

**Interpretation**:
- Higher lead times = better early warning
- Mean lead time shows average prediction advance
- Distribution shows consistency

**Location**: `model/artifacts/{version}/plots/lead_time_distribution.png`

#### 7. SHAP Feature Importance (Optional)

**Description**: SHAP (SHapley Additive exPlanations) values showing feature contributions.

**Visualization Details**:
- **Type**: Waterfall or Bar Chart
- **X-Axis**: SHAP Value (contribution to prediction)
- **Y-Axis**: Feature Names
- **Colors**: Red (increases risk), Blue (decreases risk)
- **Size**: 12x8 inches

**Interpretation**:
- Positive SHAP values increase failure risk
- Negative SHAP values decrease failure risk
- Magnitude shows feature impact

**Location**: `model/artifacts/{version}/shap/shap_summary.png`

---

## Model Integration

### API Integration

Both models are served through the **ML Model API** (Port 8080):

#### DDoS Detection Endpoint

```http
POST /predict
Content-Type: application/json

{
  "metrics": {
    "Protocol": 6,
    "Flow Duration": 120.5,
    "Total Fwd Packets": 1000,
    ...
  }
}
```

**Response**:
```json
{
  "prediction": 0.87,
  "is_ddos": true,
  "confidence": 0.74,
  "risk_level": "High"
}
```

#### Predictive Maintenance Endpoints

```http
GET /predict-failure-risk
```

**Response**:
```json
{
  "risk_score": 0.75,
  "risk_percentage": 75.0,
  "has_early_warning": true
}
```

```http
GET /predict-time-to-failure
```

**Response**:
```json
{
  "hours_until_failure": 4.5,
  "predicted_failure_time": "2025-01-19T16:30:00"
}
```

### Dashboard Integration

Models are integrated into the Healing Dashboard:

1. **Real-time Predictions**: Updated every 60 seconds
2. **Visual Indicators**: Color-coded risk levels
3. **Alert System**: Notifications for high-risk predictions
4. **Historical Charts**: Trend visualization over time

### Service Dependencies

```
Healing Dashboard (Port 5001)
    â”‚
    â–¼
Monitoring Server (Port 5000)
    â”‚
    â–¼
ML Model API (Port 8080)
    â”‚
    â”œâ”€ DDoS Model (TensorFlow)
    â””â”€ Predictive Model (XGBoost)
```

---

## Model Monitoring

### Health Checks

**Model Health Endpoint**:
```http
GET /health
```

**Response**:
```json
{
  "status": "healthy",
  "ddos_model": {
    "loaded": true,
    "version": "1.0",
    "last_updated": "2025-01-19T10:00:00"
  },
  "predictive_model": {
    "loaded": true,
    "version": "v20250119_120000",
    "last_updated": "2025-01-19T12:00:00"
  }
}
```

### Performance Monitoring

**Metrics Endpoint**:
```http
GET /metrics
```

**Response**:
```json
{
  "ddos_model": {
    "total_predictions": 10000,
    "avg_latency_ms": 45,
    "accuracy": 0.93
  },
  "predictive_model": {
    "total_predictions": 5000,
    "avg_latency_ms": 120,
    "early_detection_rate": 0.85
  }
}
```

### Model Versioning

- **DDoS Model**: Single version (`ddos_model.keras`)
- **Predictive Model**: Versioned artifacts (`v{timestamp}/`)
- **Latest Symlink**: `artifacts/latest/` points to most recent version

---

## Training & Retraining

### Initial Training

#### DDoS Model Training

```bash
cd model
python3 train_ddos_model.py
```

**Output**: `ddos_model.keras`

#### Predictive Model Training

```bash
cd model
python3 train_xgboost_model.py --enable-regression
```

**Output**: `artifacts/v{timestamp}/`

### Automated Retraining

#### Data Collection

```bash
python3 collect_training_data.py --duration 168 --interval 60
# Collects for 1 week, every minute
```

#### Automated Retraining

```bash
python3 automated_retraining.py
```

**Triggers**:
- Weekly schedule (Sundays at 2 AM)
- Performance degradation
- New data availability

### Model Validation

```bash
python3 verify_model.py
```

**Checks**:
- Model file existence
- Prediction functionality
- Performance metrics
- Integration readiness

---

## Best Practices

### Model Deployment

1. **Version Control**: Always version model artifacts
2. **Backup**: Keep previous model versions
3. **Testing**: Validate models before deployment
4. **Monitoring**: Track prediction performance
5. **Retraining**: Schedule regular retraining

### Performance Optimization

1. **Feature Selection**: Use only relevant features
2. **Hyperparameter Tuning**: Optimize model parameters
3. **Data Quality**: Ensure clean, representative data
4. **Regular Updates**: Retrain with new data
5. **A/B Testing**: Compare model versions

### Troubleshooting

**Model Not Loading**:
- Check file paths
- Verify model format
- Check dependencies

**Low Accuracy**:
- Collect more training data
- Feature engineering
- Hyperparameter tuning
- Check data quality

**High Latency**:
- Optimize feature extraction
- Model quantization
- Hardware acceleration
- Caching predictions

---

**Document Version**: 1.0  
**Last Updated**: 2025-01-19  
**Maintained By**: Heal-X-Bot Development Team

